#!/usr/bin/env python3

"""
Heimdall SBOM Performance Analyzer

This script analyzes performance logs generated by the profiling script
and provides insights into bottlenecks and optimization opportunities.
"""

import os
import sys
import json
import re
import argparse
from pathlib import Path
from collections import defaultdict
import statistics

class PerformanceAnalyzer:
    def __init__(self, profile_dir):
        self.profile_dir = Path(profile_dir)
        self.results = defaultdict(dict)
        
    def analyze_timing_logs(self):
        """Analyze timing logs to identify bottlenecks."""
        print("=== Timing Analysis ===")
        
        timing_files = list(self.profile_dir.glob("*_timing.log"))
        if not timing_files:
            print("No timing logs found.")
            return
            
        for log_file in timing_files:
            print(f"\nAnalyzing: {log_file.name}")
            
            with open(log_file, 'r') as f:
                content = f.read()
                
            # Extract timing information
            time_pattern = r"Total time: ([\d.]+) seconds"
            time_match = re.search(time_pattern, content)
            if time_match:
                duration = float(time_match.group(1))
                print(f"  Total duration: {duration:.2f} seconds")
                
                if duration > 5.0:
                    print(f"  ⚠️  SLOW: Duration exceeds 5 seconds")
                elif duration > 2.0:
                    print(f"  ⚠️  MODERATE: Duration exceeds 2 seconds")
                else:
                    print(f"  ✅ FAST: Duration under 2 seconds")
                    
            # Extract system time information
            user_time = re.search(r"User time \(seconds\): ([\d.]+)", content)
            sys_time = re.search(r"System time \(seconds\): ([\d.]+)", content)
            
            if user_time and sys_time:
                user = float(user_time.group(1))
                sys = float(sys_time.group(1))
                cpu_ratio = (user + sys) / duration if duration > 0 else 0
                print(f"  CPU usage: {cpu_ratio:.2%}")
                
                if cpu_ratio < 0.5:
                    print(f"  ⚠️  I/O BOUND: Low CPU usage suggests I/O bottlenecks")
                else:
                    print(f"  ✅ CPU BOUND: High CPU usage suggests computational bottlenecks")
                    
    def analyze_system_calls(self):
        """Analyze system call logs to identify I/O patterns."""
        print("\n=== System Call Analysis ===")
        
        strace_files = list(self.profile_dir.glob("*_strace.log"))
        if not strace_files:
            print("No system call logs found.")
            return
            
        for log_file in strace_files:
            print(f"\nAnalyzing: {log_file.name}")
            
            with open(log_file, 'r') as f:
                content = f.read()
                
            # Count system calls
            call_counts = defaultdict(int)
            for line in content.split('\n'):
                if 'syscall' in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        syscall = parts[1]
                        call_counts[syscall] += 1
                        
            # Find most frequent calls
            if call_counts:
                most_frequent = sorted(call_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                print("  Most frequent system calls:")
                for syscall, count in most_frequent:
                    print(f"    {syscall}: {count} calls")
                    
                # Check for potential bottlenecks
                if call_counts.get('read', 0) > 100:
                    print(f"  ⚠️  HIGH I/O: Many read() calls ({call_counts['read']})")
                if call_counts.get('open', 0) > 50:
                    print(f"  ⚠️  MANY FILES: Many open() calls ({call_counts['open']})")
                    
    def analyze_memory_usage(self):
        """Analyze memory usage logs."""
        print("\n=== Memory Usage Analysis ===")
        
        memory_files = list(self.profile_dir.glob("*_memory.log"))
        if not memory_files:
            print("No memory usage logs found.")
            return
            
        for log_file in memory_files:
            print(f"\nAnalyzing: {log_file.name}")
            
            with open(log_file, 'r') as f:
                content = f.read()
                
            # Extract peak memory usage
            peak_pattern = r"Peak heap: ([\d,]+) bytes"
            peak_match = re.search(peak_pattern, content)
            if peak_match:
                peak_bytes = int(peak_match.group(1).replace(',', ''))
                peak_mb = peak_bytes / (1024 * 1024)
                print(f"  Peak memory usage: {peak_mb:.1f} MB")
                
                if peak_mb > 100:
                    print(f"  ⚠️  HIGH MEMORY: Peak usage exceeds 100 MB")
                elif peak_mb > 50:
                    print(f"  ⚠️  MODERATE MEMORY: Peak usage exceeds 50 MB")
                else:
                    print(f"  ✅ LOW MEMORY: Peak usage under 50 MB")
                    
    def analyze_component_logs(self):
        """Analyze component-level performance logs."""
        print("\n=== Component Analysis ===")
        
        component_files = list(self.profile_dir.glob("*_components.log"))
        if not component_files:
            print("No component logs found.")
            return
            
        for log_file in component_files:
            print(f"\nAnalyzing: {log_file.name}")
            
            with open(log_file, 'r') as f:
                content = f.read()
                
            # Extract processing time
            time_pattern = r"Total processing time: ([\d.]+) seconds"
            time_match = re.search(time_pattern, content)
            if time_match:
                duration = float(time_match.group(1))
                print(f"  Total processing time: {duration:.2f} seconds")
                
            # Extract file size
            size_pattern = r"Output file size: ([\d,]+) bytes"
            size_match = re.search(size_pattern, content)
            if size_match:
                size_bytes = int(size_match.group(1).replace(',', ''))
                size_kb = size_bytes / 1024
                print(f"  Output file size: {size_kb:.1f} KB")
                
            # Extract component count
            count_pattern = r"Component count: (\d+)"
            count_match = re.search(count_pattern, content)
            if count_match:
                count = int(count_match.group(1))
                print(f"  Component count: {count}")
                
    def generate_recommendations(self):
        """Generate optimization recommendations based on analysis."""
        print("\n=== Optimization Recommendations ===")
        
        recommendations = []
        
        # Check for common issues
        timing_files = list(self.profile_dir.glob("*_timing.log"))
        if timing_files:
            slow_count = 0
            for log_file in timing_files:
                with open(log_file, 'r') as f:
                    content = f.read()
                time_match = re.search(r"Total time: ([\d.]+) seconds", content)
                if time_match and float(time_match.group(1)) > 2.0:
                    slow_count += 1
                    
            if slow_count > 0:
                recommendations.append({
                    'issue': 'Slow processing times detected',
                    'recommendation': 'Consider using --no-debug-info flag to disable DWARF extraction',
                    'priority': 'HIGH'
                })
                
        # Check for memory issues
        memory_files = list(self.profile_dir.glob("*_memory.log"))
        if memory_files:
            high_memory_count = 0
            for log_file in memory_files:
                with open(log_file, 'r') as f:
                    content = f.read()
                peak_match = re.search(r"Peak heap: ([\d,]+) bytes", content)
                if peak_match:
                    peak_mb = int(peak_match.group(1).replace(',', '')) / (1024 * 1024)
                    if peak_mb > 100:
                        high_memory_count += 1
                        
            if high_memory_count > 0:
                recommendations.append({
                    'issue': 'High memory usage detected',
                    'recommendation': 'Consider processing smaller batches or implementing memory limits',
                    'priority': 'MEDIUM'
                })
                
        # Check for I/O bottlenecks
        strace_files = list(self.profile_dir.glob("*_strace.log"))
        if strace_files:
            high_io_count = 0
            for log_file in strace_files:
                with open(log_file, 'r') as f:
                    content = f.read()
                read_count = len(re.findall(r'read\(', content))
                if read_count > 100:
                    high_io_count += 1
                    
            if high_io_count > 0:
                recommendations.append({
                    'issue': 'High I/O activity detected',
                    'recommendation': 'Consider using SSD storage or implementing I/O caching',
                    'priority': 'MEDIUM'
                })
                
        # Default recommendations
        if not recommendations:
            recommendations.append({
                'issue': 'No major issues detected',
                'recommendation': 'Performance appears acceptable. Monitor for larger files.',
                'priority': 'LOW'
            })
            
        # Print recommendations
        for i, rec in enumerate(recommendations, 1):
            priority_icon = {
                'HIGH': '🔴',
                'MEDIUM': '🟡', 
                'LOW': '🟢'
            }.get(rec['priority'], '⚪')
            
            print(f"\n{i}. {priority_icon} {rec['issue']}")
            print(f"   Recommendation: {rec['recommendation']}")
            print(f"   Priority: {rec['priority']}")
            
    def generate_summary(self):
        """Generate a summary of all findings."""
        print("\n=== Performance Summary ===")
        
        # Count files by type
        timing_count = len(list(self.profile_dir.glob("*_timing.log")))
        strace_count = len(list(self.profile_dir.glob("*_strace.log")))
        memory_count = len(list(self.profile_dir.glob("*_memory.log")))
        component_count = len(list(self.profile_dir.glob("*_components.log")))
        
        print(f"Timing logs: {timing_count}")
        print(f"System call logs: {strace_count}")
        print(f"Memory logs: {memory_count}")
        print(f"Component logs: {component_count}")
        
        # Overall assessment
        if timing_count > 0:
            print("\nOverall Assessment:")
            print("✅ Performance analysis completed successfully")
            print("📊 Check individual log files for detailed metrics")
            print("💡 Review recommendations for optimization opportunities")
        else:
            print("\n⚠️  No performance data found")
            print("   Run the profiling script first to generate data")

def main():
    parser = argparse.ArgumentParser(description='Analyze Heimdall SBOM performance logs')
    parser.add_argument('profile_dir', help='Directory containing performance logs')
    parser.add_argument('--output', '-o', help='Output file for analysis results')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.profile_dir):
        print(f"Error: Profile directory '{args.profile_dir}' does not exist")
        sys.exit(1)
        
    analyzer = PerformanceAnalyzer(args.profile_dir)
    
    # Capture output if requested
    if args.output:
        import io
        import contextlib
        
        output_buffer = io.StringIO()
        with contextlib.redirect_stdout(output_buffer):
            analyzer.analyze_timing_logs()
            analyzer.analyze_system_calls()
            analyzer.analyze_memory_usage()
            analyzer.analyze_component_logs()
            analyzer.generate_recommendations()
            analyzer.generate_summary()
            
        with open(args.output, 'w') as f:
            f.write(output_buffer.getvalue())
        print(f"Analysis results written to: {args.output}")
    else:
        analyzer.analyze_timing_logs()
        analyzer.analyze_system_calls()
        analyzer.analyze_memory_usage()
        analyzer.analyze_component_logs()
        analyzer.generate_recommendations()
        analyzer.generate_summary()

if __name__ == '__main__':
    main() 